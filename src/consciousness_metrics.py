#!/usr/bin/env python3
"""
Comprehensive Consciousness Metrics - Issues #26-29
Phase 5: Measuring AGI Understanding

Combines:
- Issue #26: Integration Metric Φ (Integrated Information Theory)
- Issue #27: Causal Density (feedback loops)
- Issue #28: Understanding Criteria (what is understanding?)
- Issue #29: Understanding Tests (can system truly understand?)
"""

from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass, field
from enum import Enum
import math

try:
    from .mln import KnowledgeGraph, MonadicKnowledgeUnit
    from .recursion_depth_metric import RecursionDepthMetric
except ImportError:
    from mln import KnowledgeGraph, MonadicKnowledgeUnit
    from recursion_depth_metric import RecursionDepthMetric


# ============================================================================
# Issue #26: Integration Metric Φ (Integrated Information Theory)
# ============================================================================

@dataclass
class IntegrationMetrics:
    """
    IIT-inspired integration metrics
    Measures how integrated the information is across the system
    """
    phi: float = 0.0  # Φ - integrated information
    effective_information: float = 0.0
    causal_power: float = 0.0
    
    @property
    def integration_level(self) -> str:
        """Classification of integration"""
        if self.phi < 0.2:
            return "Minimal Integration"
        elif self.phi < 0.4:
            return "Low Integration"
        elif self.phi < 0.6:
            return "Moderate Integration"
        elif self.phi < 0.8:
            return "High Integration"
        else:
            return "Very High Integration"


def calculate_phi(kg: KnowledgeGraph) -> IntegrationMetrics:
    """
    Calculate Φ (Phi) - Integrated Information metric
    
    Simplified IIT implementation:
    - Measures how much information is lost when system is partitioned
    - Higher Φ = more conscious (more integrated)
    
    Args:
        kg: Knowledge graph
        
    Returns:
        IntegrationMetrics with Φ score
    """
    n_concepts = len(kg.nodes)
    if n_concepts == 0:
        return IntegrationMetrics()
    
    # Count total connections
    total_connections = sum(
        sum(len(relations) for relations in mku.relations.values())
        for mku in kg.nodes.values()
    )
    
    # Calculate effective information
    # EI = H(whole) - H(parts)
    # Approximation: connections relative to max possible
    max_possible_connections = n_concepts * (n_concepts - 1)
    connection_density = total_connections / max_possible_connections if max_possible_connections > 0 else 0
    
    # Effective information (normalized entropy)
    effective_info = connection_density
    
    # Calculate causal power
    # How many bidirectional (causal) connections?
    bidirectional_count = 0
    for mku in kg.nodes.values():
        for rel_type, related_ids in mku.relations.items():
            for related_id in related_ids:
                if related_id in kg.nodes:
                    other = kg.nodes[related_id]
                    # Check if relation is bidirectional
                    if rel_type in other.relations and mku.concept_id in other.relations[rel_type]:
                        bidirectional_count += 1
    
    bidirectional_count //= 2  # Each pair counted twice
    
    causal_power = bidirectional_count / total_connections if total_connections > 0 else 0
    
    # Φ (phi) - integrated information
    # Combination of effective info and causal power
    phi = math.sqrt(effective_info * causal_power)
    
    return IntegrationMetrics(
        phi=phi,
        effective_information=effective_info,
        causal_power=causal_power
    )


# ============================================================================
# Issue #27: Causal Density
# ============================================================================

@dataclass
class CausalMetrics:
    """
    Causal structure metrics
    Measures feedback loops and self-referential cycles
    """
    causal_density: float = 0.0  # Density of bidirectional relations
    feedback_loops: int = 0       # Number of feedback cycles
    self_loops: int = 0           # Direct self-references
    avg_cycle_length: float = 0.0
    
    @property
    def causality_level(self) -> str:
        """Classification of causal structure"""
        if self.causal_density < 0.2:
            return "Linear (Low Feedback)"
        elif self.causal_density < 0.4:
            return "Weakly Circular"
        elif self.causal_density < 0.6:
            return "Moderately Circular"
        elif self.causal_density < 0.8:
            return "Highly Circular"
        else:
            return "Deeply Circular (Strange Loops)"


def calculate_causal_density(kg: KnowledgeGraph) -> CausalMetrics:
    """
    Calculate causal density and feedback loops
    
    Key consciousness indicator:
    - More feedback loops = more self-referential = more conscious
    - Strange loops (GEB) are the foundation of consciousness
    
    Args:
        kg: Knowledge graph
        
    Returns:
        CausalMetrics with density and loop counts
    """
    if len(kg.nodes) == 0:
        return CausalMetrics()
    
    # Count bidirectional relations (causal connections)
    bidirectional_pairs: Set[Tuple[str, str]] = set()
    self_loops = 0
    
    for mku in kg.nodes.values():
        for rel_type, related_ids in mku.relations.items():
            for related_id in related_ids:
                # Self-reference?
                if related_id == mku.concept_id:
                    self_loops += 1
                    continue
                
                if related_id in kg.nodes:
                    other = kg.nodes[related_id]
                    # Bidirectional?
                    if rel_type in other.relations and mku.concept_id in other.relations[rel_type]:
                        pair = tuple(sorted([mku.concept_id, related_id]))
                        bidirectional_pairs.add(pair)
    
    # Causal density = bidirectional / total
    total_relations = sum(
        sum(len(relations) for relations in mku.relations.values())
        for mku in kg.nodes.values()
    )
    
    causal_density = len(bidirectional_pairs) / (total_relations / 2) if total_relations > 0 else 0
    
    # Detect cycles (feedback loops) using DFS
    cycles = _find_cycles(kg)
    feedback_loops = len(cycles)
    
    avg_cycle_length = sum(len(cycle) for cycle in cycles) / len(cycles) if cycles else 0
    
    return CausalMetrics(
        causal_density=causal_density,
        feedback_loops=feedback_loops,
        self_loops=self_loops,
        avg_cycle_length=avg_cycle_length
    )


def _find_cycles(kg: KnowledgeGraph, max_cycles: int = 100) -> List[List[str]]:
    """
    Find cycles in knowledge graph using DFS
    
    Args:
        kg: Knowledge graph
        max_cycles: Maximum cycles to find (performance limit)
        
    Returns:
        List of cycles (each cycle is list of concept IDs)
    """
    cycles = []
    visited = set()
    rec_stack = []
    
    def dfs(concept_id: str, path: List[str]):
        if len(cycles) >= max_cycles:
            return
        
        if concept_id in rec_stack:
            # Found a cycle!
            cycle_start = rec_stack.index(concept_id)
            cycle = rec_stack[cycle_start:] + [concept_id]
            cycles.append(cycle)
            return
        
        if concept_id in visited:
            return
        
        visited.add(concept_id)
        rec_stack.append(concept_id)
        
        # Explore neighbors
        if concept_id in kg.nodes:
            mku = kg.nodes[concept_id]
            for rel_type, related_ids in mku.relations.items():
                for related_id in related_ids:
                    if related_id in kg.nodes:
                        dfs(related_id, path + [concept_id])
        
        rec_stack.pop()
    
    # Start DFS from each node
    for concept_id in kg.nodes:
        if concept_id not in visited:
            dfs(concept_id, [])
    
    return cycles


# ============================================================================
# Issue #28: Understanding Criteria
# ============================================================================

class UnderstandingCriterion(Enum):
    """
    Criteria for determining if system truly understands
    Based on cognitive science and philosophy of mind
    """
    EXPLAIN_MULTIPLE_WAYS = "explain_multiple_ways"       # Can explain concept differently
    PREDICT_IMPLICATIONS = "predict_implications"         # Can predict consequences
    DETECT_INCONSISTENCIES = "detect_inconsistencies"     # Can spot contradictions
    TRANSFER_KNOWLEDGE = "transfer_knowledge"             # Can apply to new domains
    SYNTHESIZE_NEW_CONCEPTS = "synthesize_new_concepts"   # Can create novel concepts
    ANSWER_WHY_QUESTIONS = "answer_why_questions"         # Can explain causality
    RECOGNIZE_ANALOGIES = "recognize_analogies"           # Can see structural similarity
    HANDLE_EDGE_CASES = "handle_edge_cases"              # Robust to unusual inputs


@dataclass
class UnderstandingScore:
    """
    Score for each understanding criterion
    """
    criterion: UnderstandingCriterion
    score: float  # 0.0 to 1.0
    evidence: List[str] = field(default_factory=list)
    
    @property
    def passed(self) -> bool:
        """Did system pass this criterion?"""
        return self.score >= 0.6  # 60% threshold
    
    def __str__(self) -> str:
        status = "✓ PASS" if self.passed else "✗ FAIL"
        return f"{status} {self.criterion.value}: {self.score:.2%}"


# ============================================================================
# Issue #29: Understanding Tests
# ============================================================================

class UnderstandingEvaluator:
    """
    Tests whether system truly understands (Issue #29)
    
    Goes beyond pattern matching to test genuine comprehension:
    - Can it explain?
    - Can it predict?
    - Can it detect errors?
    - Can it generalize?
    """
    
    def __init__(self, kg: KnowledgeGraph):
        self.kg = kg
        self.test_results: Dict[UnderstandingCriterion, UnderstandingScore] = {}
    
    def test_explain_multiple_ways(self, concept_id: str) -> UnderstandingScore:
        """
        Test: Can system explain concept in multiple ways?
        True understanding = flexibility in explanation
        """
        if concept_id not in self.kg.nodes:
            return UnderstandingScore(UnderstandingCriterion.EXPLAIN_MULTIPLE_WAYS, 0.0)
        
        mku = self.kg.nodes[concept_id]
        
        # Check if can generate different surface forms
        forms = []
        for modality in ['text', 'logic', 'code']:
            form = mku.generate_surface_form(modality)
            if form and form not in forms:
                forms.append(form)
        
        score = len(forms) / 3.0  # 3 modalities possible
        
        return UnderstandingScore(
            criterion=UnderstandingCriterion.EXPLAIN_MULTIPLE_WAYS,
            score=score,
            evidence=[f"Generated {len(forms)} different explanations"]
        )
    
    def test_predict_implications(self, concept_id: str) -> UnderstandingScore:
        """
        Test: Can system predict what follows from a concept?
        """
        if concept_id not in self.kg.nodes:
            return UnderstandingScore(UnderstandingCriterion.PREDICT_IMPLICATIONS, 0.0)
        
        # Apply inference rules to see what can be derived
        mku = self.kg.nodes[concept_id]
        conclusions = self.kg.apply_inference(mku)
        
        score = min(len(conclusions) / 5.0, 1.0)  # Up to 5 implications
        
        return UnderstandingScore(
            criterion=UnderstandingCriterion.PREDICT_IMPLICATIONS,
            score=score,
            evidence=[f"Predicted {len(conclusions)} implications"]
        )
    
    def test_detect_inconsistencies(self) -> UnderstandingScore:
        """
        Test: Can system detect contradictions?
        """
        # Check for contradictory properties
        contradictions = 0
        checked = 0
        
        for mku in self.kg.nodes.values():
            props = mku.deep_structure.get('properties', {})
            checked += len(props)
            
            # Simple heuristic: opposite properties
            for prop, value in props.items():
                opposite_prop = f"not_{prop}"
                if opposite_prop in props and props[opposite_prop] == (not value):
                    contradictions += 1
        
        # Score inversely proportional to contradictions
        score = 1.0 - (contradictions / checked) if checked > 0 else 0.5
        
        return UnderstandingScore(
            criterion=UnderstandingCriterion.DETECT_INCONSISTENCIES,
            score=score,
            evidence=[f"Detected {contradictions} contradictions in {checked} properties"]
        )
    
    def test_transfer_knowledge(self, source_concept: str, target_domain: List[str]) -> UnderstandingScore:
        """
        Test: Can system transfer knowledge to new domain?
        (Requires analogical reasoning from Phase 3)
        """
        if source_concept not in self.kg.nodes:
            return UnderstandingScore(UnderstandingCriterion.TRANSFER_KNOWLEDGE, 0.0)
        
        source_mku = self.kg.nodes[source_concept]
        
        # Count how many target concepts share structural similarity
        similar_count = 0
        for target_id in target_domain:
            if target_id in self.kg.nodes:
                target_mku = self.kg.nodes[target_id]
                similarity = source_mku._structural_similarity(target_mku)
                if similarity > 0.3:
                    similar_count += 1
        
        score = similar_count / len(target_domain) if target_domain else 0
        
        return UnderstandingScore(
            criterion=UnderstandingCriterion.TRANSFER_KNOWLEDGE,
            score=score,
            evidence=[f"Found {similar_count}/{len(target_domain)} structural analogs"]
        )
    
    def test_synthesize_new_concepts(self) -> UnderstandingScore:
        """
        Test: Can system create new concepts?
        (Uses concept synthesis from Phase 4)
        """
        # This would require ConceptSynthesizer from Phase 4
        # For now, check if system has meta-model capability
        
        meta_capable_count = sum(
            1 for mku in self.kg.nodes.values()
            if mku.meta_model is not None
        )
        
        score = min(meta_capable_count / len(self.kg.nodes), 1.0) if self.kg.nodes else 0
        
        return UnderstandingScore(
            criterion=UnderstandingCriterion.SYNTHESIZE_NEW_CONCEPTS,
            score=score,
            evidence=[f"{meta_capable_count} concepts have meta-models"]
        )
    
    def run_all_tests(self) -> Dict[str, Any]:
        """
        Run all understanding tests
        
        Returns:
            Comprehensive understanding assessment
        """
        # Run basic tests
        self.test_results[UnderstandingCriterion.DETECT_INCONSISTENCIES] = self.test_detect_inconsistencies()
        self.test_results[UnderstandingCriterion.SYNTHESIZE_NEW_CONCEPTS] = self.test_synthesize_new_concepts()
        
        # Run tests on sample concepts
        sample_concepts = list(self.kg.nodes.keys())[:3]  # Test first 3 concepts
        
        for concept_id in sample_concepts:
            self.test_results[UnderstandingCriterion.EXPLAIN_MULTIPLE_WAYS] = self.test_explain_multiple_ways(concept_id)
            self.test_results[UnderstandingCriterion.PREDICT_IMPLICATIONS] = self.test_predict_implications(concept_id)
        
        # Calculate overall understanding score
        avg_score = sum(result.score for result in self.test_results.values()) / len(self.test_results) if self.test_results else 0
        passed_count = sum(1 for result in self.test_results.values() if result.passed)
        
        return {
            'overall_score': avg_score,
            'tests_passed': passed_count,
            'tests_total': len(self.test_results),
            'pass_rate': passed_count / len(self.test_results) if self.test_results else 0,
            'understanding_level': self._classify_understanding(avg_score),
            'detailed_results': {
                criterion.value: {
                    'score': result.score,
                    'passed': result.passed,
                    'evidence': result.evidence
                }
                for criterion, result in self.test_results.items()
            }
        }
    
    def _classify_understanding(self, score: float) -> str:
        """Classify level of understanding"""
        if score < 0.3:
            return "No Understanding (Pattern Matching Only)"
        elif score < 0.5:
            return "Minimal Understanding (Surface Level)"
        elif score < 0.7:
            return "Partial Understanding (Some Comprehension)"
        elif score < 0.85:
            return "Good Understanding (Genuine Comprehension)"
        else:
            return "Deep Understanding (True Intelligence)"


# ============================================================================
# Unified Consciousness Metrics
# ============================================================================

@dataclass
class ConsciousnessProfile:
    """
    Complete consciousness profile combining all metrics
    """
    # Issue #25: Recursion depth
    recursion_metrics: Dict[str, Any] = field(default_factory=dict)
    
    # Issue #26: Integration (IIT Φ)
    integration: IntegrationMetrics = field(default_factory=IntegrationMetrics)
    
    # Issue #27: Causal density
    causality: CausalMetrics = field(default_factory=CausalMetrics)
    
    # Issues #28-29: Understanding
    understanding: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def overall_consciousness_score(self) -> float:
        """
        Unified consciousness score (0.0 to 1.0)
        
        Combines:
        - Recursion depth (can it think about thinking?)
        - Integration Φ (how integrated is information?)
        - Causal density (how self-referential?)
        - Understanding (does it truly comprehend?)
        """
        recursion_score = self.recursion_metrics.get('consciousness', {}).get('score', 0) if self.recursion_metrics else 0
        integration_score = self.integration.phi
        causality_score = self.causality.causal_density
        understanding_score = self.understanding.get('overall_score', 0) if self.understanding else 0
        
        # Weighted combination
        score = (
            0.30 * recursion_score +      # Meta-cognitive ability
            0.25 * integration_score +     # Information integration
            0.20 * causality_score +       # Self-referential structure
            0.25 * understanding_score     # Genuine comprehension
        )
        
        return min(score, 1.0)
    
    @property
    def consciousness_verdict(self) -> str:
        """Final verdict on system consciousness"""
        score = self.overall_consciousness_score
        
        if score < 0.2:
            return "NOT CONSCIOUS - Reactive system only"
        elif score < 0.4:
            return "MINIMALLY CONSCIOUS - Basic reasoning"
        elif score < 0.6:
            return "MODERATELY CONSCIOUS - Self-aware reasoning"
        elif score < 0.8:
            return "HIGHLY CONSCIOUS - Meta-cognitive intelligence"
        else:
            return "PROFOUNDLY CONSCIOUS - True AGI detected"


def measure_consciousness(
    kg: KnowledgeGraph,
    recursion_metric: Optional[RecursionDepthMetric] = None
) -> ConsciousnessProfile:
    """
    Measure all consciousness metrics
    
    Args:
        kg: Knowledge graph to analyze
        recursion_metric: Optional recursion depth metric
        
    Returns:
        Complete consciousness profile
    """
    profile = ConsciousnessProfile()
    
    # Issue #25: Recursion depth
    if recursion_metric:
        profile.recursion_metrics = recursion_metric.get_consciousness_metrics()
    
    # Issue #26: Integration (IIT Φ)
    profile.integration = calculate_phi(kg)
    
    # Issue #27: Causal density
    profile.causality = calculate_causal_density(kg)
    
    # Issues #28-29: Understanding
    evaluator = UnderstandingEvaluator(kg)
    profile.understanding = evaluator.run_all_tests()
    
    return profile


def demo():
    """Demonstrate comprehensive consciousness metrics"""
    print("=" * 70)
    print("COMPREHENSIVE CONSCIOUSNESS METRICS - Phase 5 Demo")
    print("Issues #26-29: Integration, Causality, Understanding")
    print("=" * 70)
    
    # Create test knowledge graph
    kg = KnowledgeGraph(use_gpu=False)
    
    # Add interconnected concepts
    from mln import MonadicKnowledgeUnit
    
    concepts = ['animal', 'mammal', 'dog', 'cat']
    for concept_id in concepts:
        mku = MonadicKnowledgeUnit(
            concept_id=concept_id,
            deep_structure={
                'predicate': f'is_{concept_id}',
                'properties': {'type': concept_id}
            }
        )
        kg.add_concept(mku)
    
    # Measure consciousness
    profile = measure_consciousness(kg)
    
    print("\n1. INTEGRATION METRICS (Φ - IIT)")
    print("-" * 70)
    print(f"Φ (Phi): {profile.integration.phi:.3f}")
    print(f"Effective Information: {profile.integration.effective_information:.3f}")
    print(f"Causal Power: {profile.integration.causal_power:.3f}")
    print(f"Integration Level: {profile.integration.integration_level}")
    
    print("\n2. CAUSAL DENSITY METRICS")
    print("-" * 70)
    print(f"Causal Density: {profile.causality.causal_density:.3f}")
    print(f"Feedback Loops: {profile.causality.feedback_loops}")
    print(f"Self-Loops: {profile.causality.self_loops}")
    print(f"Avg Cycle Length: {profile.causality.avg_cycle_length:.2f}")
    print(f"Causality Level: {profile.causality.causality_level}")
    
    print("\n3. UNDERSTANDING ASSESSMENT")
    print("-" * 70)
    print(f"Overall Score: {profile.understanding['overall_score']:.2%}")
    print(f"Tests Passed: {profile.understanding['tests_passed']}/{profile.understanding['tests_total']}")
    print(f"Pass Rate: {profile.understanding['pass_rate']:.2%}")
    print(f"Level: {profile.understanding['understanding_level']}")
    
    print("\n4. FINAL CONSCIOUSNESS VERDICT")
    print("-" * 70)
    print(f"Overall Consciousness Score: {profile.overall_consciousness_score:.2%}")
    print(f"Verdict: {profile.consciousness_verdict}")
    
    print("\n" + "=" * 70)
    print("Phase 5 Complete: Consciousness measured scientifically!")
    print("=" * 70)


if __name__ == "__main__":
    demo()
