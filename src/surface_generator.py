#!/usr/bin/env python3
"""
Surface Generation Layer (Optional LLM Integration)

Implements Chomsky's separation of deep structure and surface structure:
- Deep structure: MKU operational semantics (symbolic, compositional)
- Surface structure: Natural language generated by LLM from deep structure

This is OPTIONAL - the system works without it using built-in surface forms.
When enabled, provides rich, diverse natural language from the same deep structure.

Supported LLM providers:
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude)
- Local models via Ollama
- HuggingFace models
"""

from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
import os
import json

# Optional LLM dependencies
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import requests
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False


@dataclass
class SurfaceGenerationConfig:
    """Configuration for LLM-powered surface generation"""
    provider: str = 'none'  # 'openai', 'anthropic', 'ollama', 'huggingface', 'none'
    model: str = 'gpt-3.5-turbo'  # Model name
    api_key: Optional[str] = None  # API key (from env if not provided)
    base_url: Optional[str] = None  # For Ollama/custom endpoints
    temperature: float = 0.7  # Generation creativity
    max_tokens: int = 150  # Max output length
    style: str = 'conversational'  # 'conversational', 'technical', 'educational', 'poetic'


class SurfaceGenerator:
    """
    Chomsky-inspired surface generator
    
    Deep structure (MKU) → Transformational rules → Surface structure (NL)
    
    The same deep structure can generate:
    - Multiple phrasings
    - Different styles (technical, casual, educational)
    - Various modalities (text, speech, visualization descriptions)
    """
    
    def __init__(self, config: Optional[SurfaceGenerationConfig] = None):
        """
        Initialize surface generator
        
        Args:
            config: Optional LLM configuration. If None, uses fallback built-in generation
        """
        self.config = config or SurfaceGenerationConfig()
        self.llm_client = None
        
        if self.config.provider != 'none':
            self._initialize_llm()
    
    def _initialize_llm(self):
        """Initialize LLM client based on provider"""
        provider = self.config.provider
        
        if provider == 'openai':
            if not OPENAI_AVAILABLE:
                raise ImportError("openai package not installed. Install: pip install openai")
            
            api_key = self.config.api_key or os.getenv('OPENAI_API_KEY')
            if not api_key:
                raise ValueError("OpenAI API key not found. Set OPENAI_API_KEY env var or pass api_key")
            
            self.llm_client = openai.OpenAI(api_key=api_key)
            print(f"SurfaceGenerator using OpenAI: {self.config.model}")
        
        elif provider == 'anthropic':
            if not ANTHROPIC_AVAILABLE:
                raise ImportError("anthropic package not installed. Install: pip install anthropic")
            
            api_key = self.config.api_key or os.getenv('ANTHROPIC_API_KEY')
            if not api_key:
                raise ValueError("Anthropic API key not found. Set ANTHROPIC_API_KEY env var")
            
            self.llm_client = anthropic.Anthropic(api_key=api_key)
            print(f"SurfaceGenerator using Anthropic: {self.config.model}")
        
        elif provider == 'ollama':
            if not OLLAMA_AVAILABLE:
                raise ImportError("requests package not installed. Install: pip install requests")
            
            self.config.base_url = self.config.base_url or 'http://localhost:11434'
            print(f"SurfaceGenerator using Ollama: {self.config.model} at {self.config.base_url}")
        
        else:
            print(f"Unknown provider: {provider}. Falling back to built-in generation")
            self.config.provider = 'none'
    
    def generate_from_mku(
        self, 
        mku_data: Dict[str, Any],
        context: Optional[str] = None,
        style: Optional[str] = None
    ) -> str:
        """
        Generate natural language surface form from MKU deep structure
        
        Args:
            mku_data: MKU deep structure (concept_id, predicate, properties, relations)
            context: Optional context for generation
            style: Override default style
            
        Returns:
            Natural language text
        """
        style = style or self.config.style
        
        if self.config.provider == 'none' or not self.llm_client:
            return self._fallback_generation(mku_data, style)
        
        # Construct prompt from deep structure
        prompt = self._construct_prompt(mku_data, context, style)
        
        # Generate using LLM
        try:
            if self.config.provider == 'openai':
                return self._generate_openai(prompt)
            elif self.config.provider == 'anthropic':
                return self._generate_anthropic(prompt)
            elif self.config.provider == 'ollama':
                return self._generate_ollama(prompt)
        except Exception as e:
            print(f"LLM generation failed: {e}. Falling back to built-in.")
            return self._fallback_generation(mku_data, style)
    
    def _construct_prompt(
        self,
        mku_data: Dict[str, Any],
        context: Optional[str],
        style: str
    ) -> str:
        """Construct prompt for LLM from MKU deep structure"""
        
        concept_id = mku_data.get('concept_id', 'unknown')
        predicate = mku_data.get('predicate', 'unknown')
        properties = mku_data.get('properties', {})
        relations = mku_data.get('relations', {})
        
        # Base prompt with deep structure
        prompt = f"""You are a surface structure generator for a knowledge representation system.
Given a deep structure (symbolic, compositional representation), generate natural language.

Deep Structure:
- Concept: {concept_id}
- Predicate: {predicate}
- Properties: {json.dumps(properties, indent=2)}
- Relations: {json.dumps({k: list(v) if isinstance(v, set) else v for k, v in relations.items()}, indent=2)}
"""
        
        if context:
            prompt += f"\nContext: {context}"
        
        # Style-specific instructions
        if style == 'conversational':
            prompt += "\n\nGenerate a friendly, conversational explanation (2-3 sentences)."
        elif style == 'technical':
            prompt += "\n\nGenerate a precise, technical description with formal terminology."
        elif style == 'educational':
            prompt += "\n\nGenerate an educational explanation suitable for learning, with examples."
        elif style == 'poetic':
            prompt += "\n\nGenerate a poetic, metaphorical description that captures the essence."
        
        prompt += "\n\nGenerate ONLY the natural language output, no preamble:"
        
        return prompt
    
    def _generate_openai(self, prompt: str) -> str:
        """Generate using OpenAI API"""
        response = self.llm_client.chat.completions.create(
            model=self.config.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens
        )
        return response.choices[0].message.content.strip()
    
    def _generate_anthropic(self, prompt: str) -> str:
        """Generate using Anthropic API"""
        message = self.llm_client.messages.create(
            model=self.config.model,
            max_tokens=self.config.max_tokens,
            temperature=self.config.temperature,
            messages=[{"role": "user", "content": prompt}]
        )
        return message.content[0].text.strip()
    
    def _generate_ollama(self, prompt: str) -> str:
        """Generate using local Ollama"""
        response = requests.post(
            f"{self.config.base_url}/api/generate",
            json={
                "model": self.config.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": self.config.temperature,
                    "num_predict": self.config.max_tokens
                }
            }
        )
        response.raise_for_status()
        return response.json()['response'].strip()
    
    def _fallback_generation(self, mku_data: Dict[str, Any], style: str) -> str:
        """Built-in generation without LLM (Chomsky's built-in transformational rules)"""
        
        concept_id = mku_data.get('concept_id', 'unknown')
        predicate = mku_data.get('predicate', 'unknown')
        properties = mku_data.get('properties', {})
        relations = mku_data.get('relations', {})
        
        if style == 'technical':
            # Formal logical style
            prop_str = ', '.join(f"{k}={v}" for k, v in properties.items())
            rel_count = sum(len(v) if isinstance(v, (set, list)) else 1 for v in relations.values())
            return f"{concept_id}: {predicate}({prop_str}) with {rel_count} relations"
        
        elif style == 'educational':
            # Explanatory style
            text = f"{concept_id.capitalize()} is a type of {predicate}. "
            if properties:
                text += f"It has these characteristics: {', '.join(f'{k} is {v}' for k, v in list(properties.items())[:3])}. "
            if relations:
                rel_types = list(relations.keys())[:2]
                text += f"It relates to other concepts through {', '.join(rel_types)} relationships."
            return text
        
        elif style == 'poetic':
            # Metaphorical style
            return f"{concept_id.capitalize()}, a {predicate} dancing in the web of knowledge, connected by invisible threads of meaning."
        
        else:  # conversational (default)
            text = f"{concept_id.capitalize()} is a {predicate}"
            if properties:
                key_props = list(properties.items())[:2]
                text += f" with {', '.join(f'{k}: {v}' for k, v in key_props)}"
            if relations:
                rel_count = sum(len(v) if isinstance(v, (set, list)) else 1 for v in relations.values())
                text += f". It has {rel_count} relationships with other concepts"
            return text + "."
    
    def generate_multiple_variants(
        self,
        mku_data: Dict[str, Any],
        num_variants: int = 3,
        styles: Optional[List[str]] = None
    ) -> List[str]:
        """
        Generate multiple surface realizations from same deep structure
        
        Demonstrates Chomsky's key insight: one meaning, many forms
        
        Args:
            mku_data: Deep structure
            num_variants: Number of variants to generate
            styles: List of styles to use (cycles through them)
            
        Returns:
            List of different surface realizations
        """
        if styles is None:
            styles = ['conversational', 'technical', 'educational']
        
        variants = []
        for i in range(num_variants):
            style = styles[i % len(styles)]
            variant = self.generate_from_mku(mku_data, style=style)
            variants.append(variant)
        
        return variants
    
    def generate_with_reasoning_chain(
        self,
        inference_chain: List[Dict[str, Any]],
        conclusion: str
    ) -> str:
        """
        Generate explanation of reasoning chain
        
        Args:
            inference_chain: List of MKU data in reasoning chain
            conclusion: Final conclusion
            
        Returns:
            Natural language explanation of reasoning
        """
        if self.config.provider == 'none' or not self.llm_client:
            return self._fallback_reasoning_chain(inference_chain, conclusion)
        
        # Construct reasoning prompt
        prompt = "Explain the following reasoning chain in natural language:\n\n"
        for i, step in enumerate(inference_chain, 1):
            concept = step.get('concept_id', 'unknown')
            prompt += f"{i}. {concept}: {step.get('predicate', 'unknown')}\n"
        prompt += f"\nConclusion: {conclusion}\n\n"
        prompt += "Generate a clear explanation of how these steps lead to the conclusion:"
        
        try:
            if self.config.provider == 'openai':
                return self._generate_openai(prompt)
            elif self.config.provider == 'anthropic':
                return self._generate_anthropic(prompt)
            elif self.config.provider == 'ollama':
                return self._generate_ollama(prompt)
        except Exception as e:
            return self._fallback_reasoning_chain(inference_chain, conclusion)
    
    def _fallback_reasoning_chain(
        self,
        inference_chain: List[Dict[str, Any]],
        conclusion: str
    ) -> str:
        """Built-in reasoning chain explanation"""
        if not inference_chain:
            return f"Direct conclusion: {conclusion}"
        
        text = "Reasoning: "
        for i, step in enumerate(inference_chain, 1):
            concept = step.get('concept_id', 'unknown')
            if i == 1:
                text += f"Starting from {concept}"
            else:
                text += f", then {concept}"
        text += f", we conclude: {conclusion}"
        return text


# Convenience function for integration
def create_surface_generator(
    provider: str = 'none',
    model: Optional[str] = None,
    **kwargs
) -> SurfaceGenerator:
    """
    Factory function to create surface generator
    
    Examples:
        # No LLM (built-in only)
        gen = create_surface_generator()
        
        # OpenAI
        gen = create_surface_generator('openai', model='gpt-4')
        
        # Local Ollama
        gen = create_surface_generator('ollama', model='llama2')
    """
    config = SurfaceGenerationConfig(provider=provider, **kwargs)
    if model:
        config.model = model
    return SurfaceGenerator(config)


if __name__ == '__main__':
    # Demo without LLM
    print("=" * 70)
    print("SURFACE GENERATOR DEMO (Built-in, no LLM)")
    print("=" * 70)
    print()
    
    gen = SurfaceGenerator()
    
    # Example MKU
    dog_mku = {
        'concept_id': 'dog',
        'predicate': 'mammal_type',
        'properties': {
            'domesticated': True,
            'barks': True,
            'loyal': True,
            'warm_blooded': True
        },
        'relations': {
            'subtype': {'mammal', 'animal'},
            'similar_to': {'cat', 'wolf'}
        }
    }
    
    print("Deep Structure:")
    print(json.dumps(dog_mku, indent=2, default=str))
    print()
    
    print("Surface Realizations:")
    print()
    
    for style in ['conversational', 'technical', 'educational', 'poetic']:
        print(f"[{style.upper()}]")
        surface = gen.generate_from_mku(dog_mku, style=style)
        print(f"  {surface}")
        print()
    
    print("\nMultiple Variants (same deep structure):")
    variants = gen.generate_multiple_variants(dog_mku, num_variants=3)
    for i, variant in enumerate(variants, 1):
        print(f"  {i}. {variant}")
    
    print()
    print("=" * 70)
    print("✓ Same deep structure → Multiple surface forms (Chomsky!)")
    print("=" * 70)
